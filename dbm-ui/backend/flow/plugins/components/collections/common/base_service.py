# -*- coding: utf-8 -*-
"""
TencentBlueKing is pleased to support the open source community by making è“é²¸æ™ºäº‘-DBç®¡ç†ç³»ç»Ÿ(BlueKing-BK-DBM) available.
Copyright (C) 2017-2023 THL A29 Limited, a Tencent company. All rights reserved.
Licensed under the MIT License (the "License"); you may not use this file except in compliance with the License.
You may obtain a copy of the License at https://opensource.org/licenses/MIT
Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
"""
import copy
import json
import logging
import re
from abc import ABCMeta
from typing import Any, Dict, List, Optional, Union

from django.utils import translation
from django.utils.translation import ugettext as _
from pipeline.core.flow.activity import Service, StaticIntervalGenerator

from backend import env
from backend.components import JobApi
from backend.core.translation.constants import Language
from backend.flow.consts import SUCCESS_LIST, WriteContextOpType

logger = logging.getLogger("flow")
cpl = re.compile("<ctx>(?P<context>.+?)</ctx>")  # éè´ªå©ªæ¨¡å¼ï¼ŒåªåŒ¹é…ç¬¬ä¸€æ¬¡å‡ºç°çš„è‡ªå®šä¹‰tag


class ServiceLogMixin:
    def log_info(self, msg: str):
        logger.info(msg, extra=self.extra_log)

    def log_error(self, msg: str):
        logger.error(msg, extra=self.extra_log)

    def log_warning(self, msg: str):
        logger.warning(msg, extra=self.extra_log)

    def log_debug(self, msg: str):
        logger.debug(msg, extra=self.extra_log)

    def log_exception(self, msg: str):
        logger.exception(msg, extra=self.extra_log)

    def log_dividing_line(self):
        logger.info("*" * 50, extra=self.extra_log)

    @property
    def runtime_attrs(self):
        """pipeline.activityå†…éƒ¨è¿è¡Œæ—¶å±æ€§"""
        return getattr(self, "_runtime_attrs", {})

    @property
    def extra_log(self):
        """Serviceè¿è¡Œæ—¶å±æ€§ï¼Œç”¨äºè·å– root_id, node_id, version_id æ³¨å…¥æ—¥å¿—"""
        return {
            "root_id": self.runtime_attrs.get("root_pipeline_id"),
            "node_id": self.runtime_attrs.get("id"),
            "version_id": self.runtime_attrs.get("version"),
        }


class BaseService(Service, ServiceLogMixin, metaclass=ABCMeta):
    """
    DB Service åŸºç±»
    """

    def active_language(self, data):
        # æ¿€æ´»å›½é™…åŒ–
        blueking_language = data.get_one_of_inputs("global_data").get("blueking_language", Language.ZH_CN.value)
        translation.activate(blueking_language)
        # self.log_info(f"System language: {blueking_language}")

    def execute(self, data, parent_data):
        self.active_language(data)

        kwargs = data.get_one_of_inputs("kwargs") or {}
        try:
            result = self._execute(data, parent_data)
            if result:
                self.log_info(_("[{}] è¿è¡ŒæˆåŠŸ").format(kwargs.get("node_name", self.__class__.__name__)))

            return result
        except Exception as e:  # pylint: disable=broad-except
            self.log_exception(_("[{}] å¤±è´¥: {}").format(kwargs.get("node_name", self.__class__.__name__), e))
            return False

    def _execute(self, data, parent_data):
        raise NotImplementedError()

    def schedule(self, data, parent_data, callback_data=None):
        self.active_language(data)

        kwargs = data.get_one_of_inputs("kwargs") or {}
        try:
            result = self._schedule(data, parent_data)
            return result
        except Exception as e:  # pylint: disable=broad-except
            self.log_exception(f"[{kwargs.get('node_name', self.__class__.__name__)}] failed: {e}")
            self.finish_schedule()
            return False

    def _schedule(self, data, parent_data, callback_data=None):
        self.finish_schedule()
        return True

    @staticmethod
    def splice_exec_ips_list(
        ticket_ips: Optional[Any] = None, pool_ips: Optional[Any] = None, parse_cloud: bool = False
    ) -> Union[List[Dict], List[str]]:
        """
        æ‹¼æ¥æ‰§è¡Œipåˆ—è¡¨
        @param ticket_ips: è¡¨ç¤ºå•æ®é¢„åˆ†é…å¥½çš„ipä¿¡æ¯
        @param pool_ips:   è¡¨ç¤ºä»èµ„æºæ± åˆ†é…åˆ°çš„ipä¿¡æ¯
        @param parse_cloud: æ˜¯å¦éœ€è¦è§£æäº‘åŒºåŸŸ TODO: å½“flowå…¼å®¹åï¼Œé»˜è®¤ä¸ºäº‘åŒºåŸŸ
        """

        def _format_to_str(ip: Union[str, dict]) -> Union[str, Dict]:
            """
            {"ip": "127.0.0.1", "bk_cloud_id": 0} -> "127.0.0.1"
            """
            if isinstance(ip, dict):
                if not parse_cloud:
                    return ip["ip"]
                return {"ip": ip["ip"], "bk_cloud_id": ip["bk_cloud_id"]}
            return ip

        def _format_to_list(ip: Optional[Union[list, str]]) -> list:
            # TODO æœ€ç»ˆåº”è¯¥ä½¿ç”¨ _format_to_dict ç»Ÿä¸€è¿”å› [{"ip": "127.0.0.1", "bk_cloud_id": 0 }]
            #  éœ€ flow æ•´ä½“é…åˆæ”¹é€ 
            if ip is None:
                return []
            if isinstance(ip, list):
                return [_format_to_str(_ip) for _ip in ip]
            return [_format_to_str(ip)]

        return _format_to_list(ticket_ips) + _format_to_list(pool_ips)


class BkJobService(BaseService, metaclass=ABCMeta):
    __need_schedule__ = True
    interval = StaticIntervalGenerator(5)

    @staticmethod
    def __status__(instance_id: str) -> Optional[Dict]:
        """
        è·å–ä»»åŠ¡çŠ¶æ€
        """
        payload = {
            "bk_biz_id": env.JOB_BLUEKING_BIZ_ID,
            "job_instance_id": instance_id,
            "return_ip_result": True,
        }
        resp = JobApi.get_job_instance_status(payload, raw=True)
        return resp

    def __log__(
        self,
        job_instance_id: int,
        step_instance_id: int,
        ip_dict: dict,
    ):
        """
        è·å–ä»»åŠ¡æ—¥å¿—
        """
        payload = {
            "bk_biz_id": env.JOB_BLUEKING_BIZ_ID,
            "job_instance_id": job_instance_id,
            "step_instance_id": step_instance_id,
        }
        return JobApi.get_job_instance_ip_log({**payload, **ip_dict}, raw=True)

    def __get_target_ip_context(
        self,
        job_instance_id: int,
        step_instance_id: int,
        ip_dict: dict,
        data,
        trans_data,
        write_payload_var: str,
        write_op: str,
    ):
        """
        å¯¹å•ä¸ªèŠ‚ç‚¹è·å–æ‰§è¡Œålogï¼Œå¹¶èµ‹å€¼ç»™å®šä¹‰å¥½æµç¨‹ä¸Šä¸‹æ–‡çš„trans_data
        write_op æ§åˆ¶å†™å…¥å˜é‡çš„æ–¹å¼ï¼Œrewriteæ˜¯é»˜è®¤å€¼ï¼Œä»£è¡¨è¦†ç›–å†™å…¥ï¼›appendä»£è¡¨ä»¥{"ip":xxx} å½¢å¼è¿½åŠ é‡Œé¢å˜é‡é‡Œé¢
        """
        resp = self.__log__(job_instance_id, step_instance_id, ip_dict)
        if not resp["result"]:
            # ç»“æœè¿”å›å¼‚å¸¸ï¼Œåˆ™å¼‚å¸¸é€€å‡º
            return False
        try:
            # ä»¥dictå½¢å¼è¿½åŠ å†™å…¥
            result = json.loads(re.search(cpl, resp["data"]["log_content"]).group("context"))
            if write_op == WriteContextOpType.APPEND.value:
                context = copy.deepcopy(getattr(trans_data, write_payload_var))
                ip = ip_dict["ip"]
                if context:
                    context[ip] = copy.deepcopy(result)
                else:
                    context = {ip: copy.deepcopy(result)}

                setattr(trans_data, write_payload_var, copy.deepcopy(context))

            else:
                # é»˜è®¤è¦†ç›–å†™å…¥
                setattr(trans_data, write_payload_var, copy.deepcopy(result))
            data.outputs["trans_data"] = trans_data

            return True

        except Exception as e:
            self.log_error(_("[å†™å…¥ä¸Šä¸‹æ–‡ç»“æœå¤±è´¥] failed: {}").format(e))
            return False

    def _schedule(self, data, parent_data, callback_data=None) -> bool:
        ext_result = data.get_one_of_outputs("ext_result")
        exec_ips = data.get_one_of_outputs("exec_ips")
        kwargs = data.get_one_of_inputs("kwargs")
        global_data = data.get_one_of_inputs("global_data")
        write_payload_var = data.get_one_of_inputs("write_payload_var")
        trans_data = data.get_one_of_inputs("trans_data")

        node_name = kwargs["node_name"]

        # åœ¨è½®è¯¢çš„æ—¶å€™ext_resultéƒ½ä¸ä¼šæ”¹å˜ï¼Œè€ƒè™‘æ”¶æ•›æ—¥å¿—
        if not kwargs.get(f"{node_name}_ext_result_cached"):
            kwargs[f"{node_name}_ext_result_cached"] = True
            self.log_info(f"[{node_name}] ext_result: {ext_result}")

        if isinstance(ext_result, bool):
            # ext_result ä¸º å¸ƒå°”ç±»å‹ è¡¨ç¤º ä¸éœ€è¦ä»»åŠ¡æ˜¯åŒæ­¥è¿›è¡Œï¼Œä¸éœ€è¦è°ƒç”¨apiå»ç›‘å¬ä»»åŠ¡çŠ¶æ€
            self.finish_schedule()
            return ext_result

        if not ext_result["result"]:
            # è°ƒç”¨ç»“æœæ£€æµ‹åˆ°å¤±è´¥
            self.log_error(f"[{node_name}] schedule  status failed: {ext_result['error']}")
            return False

        job_instance_id = ext_result["data"]["job_instance_id"]
        resp = self.__status__(job_instance_id)

        # è·å–ä»»åŠ¡çŠ¶æ€ï¼š
        # """
        # 1.æœªæ‰§è¡Œ; 2.æ­£åœ¨æ‰§è¡Œ; 3.æ‰§è¡ŒæˆåŠŸ; 4.æ‰§è¡Œå¤±è´¥; 5.è·³è¿‡; 6.å¿½ç•¥é”™è¯¯;
        # 7.ç­‰å¾…ç”¨æˆ·; 8.æ‰‹åŠ¨ç»“æŸ; 9.çŠ¶æ€å¼‚å¸¸; 10.æ­¥éª¤å¼ºåˆ¶ç»ˆæ­¢ä¸­; 11.æ­¥éª¤å¼ºåˆ¶ç»ˆæ­¢æˆåŠŸ; 12.æ­¥éª¤å¼ºåˆ¶ç»ˆæ­¢å¤±è´¥
        # """
        if not (resp["result"] and resp["data"]["finished"]):
            self.log_info(_("[{}] ä»»åŠ¡æ­£åœ¨æ‰§è¡ŒğŸ¤”").format(node_name))
            return True

        job_status = resp["data"]["job_instance"]["status"]
        step_instance_id = resp["data"]["step_instance_list"][0]["step_instance_id"]
        ip_dict = {"bk_cloud_id": kwargs["bk_cloud_id"], "ip": exec_ips[0]} if exec_ips else {}

        if job_status not in SUCCESS_LIST:
            self.log_info("{} job status: {}".format(node_name, resp))
            self.log_info(_("[{}]  ä»»åŠ¡è°ƒåº¦å¤±è´¥ğŸ˜±").format(node_name))

            # è½¬è½½jobè„šæœ¬èŠ‚ç‚¹æŠ¥é”™æ—¥å¿—
            if ip_dict:
                resp = self.__log__(job_instance_id, step_instance_id, ip_dict)
                if resp.get("result"):
                    self.log_error(resp["data"]["log_content"])

            self.finish_schedule()
            return False

        self.log_info(_("[{}]ä»»åŠ¡è°ƒåº¦æˆåŠŸğŸ¥³ï¸").format(node_name))
        if not write_payload_var:
            self.finish_schedule()
            return True

        # å†™å…¥ä¸Šä¸‹æ–‡ï¼Œç›®å‰å¦‚æœä¼ å…¥çš„ip_liståªæ”¯æŒä¸€ç»„ipï¼Œå¤šç»„ipä¼šå­˜åœ¨é—®é¢˜,å› ä¸ºè¿™é‡Œåªæ‹¿ç¬¬ä¸€ä¸ªæ‰§è¡Œipæ¥æ‹¼æ¥ç»“æœåˆ°å¯¹åº”çš„ä¸Šä¸‹æ–‡å˜é‡
        self.log_info(_("[{}]è¯¥èŠ‚ç‚¹éœ€è¦è·å–æ‰§è¡Œåæ—¥å¿—ï¼Œèµ‹å€¼åˆ°trans_data").format(node_name))
        self.log_info(exec_ips)
        if not self.__get_target_ip_context(
            job_instance_id=job_instance_id,
            step_instance_id=step_instance_id,
            ip_dict=ip_dict,
            data=data,
            trans_data=trans_data,
            write_payload_var=write_payload_var,
            write_op=kwargs.get("write_op", WriteContextOpType.REWRITE.value),
        ):
            self.log_info(_("[{}] è·å–æ‰§è¡Œåæ—¥å¿—å¤±è´¥ï¼Œè·å–ip[{}]").format(node_name, exec_ips[0]))
            self.finish_schedule()
            return False

        self.finish_schedule()
        return True
