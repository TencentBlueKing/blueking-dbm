# -*- coding: utf-8 -*-
"""
TencentBlueKing is pleased to support the open source community by making è“é²¸æ™ºäº‘-DBç®¡ç†ç³»ç»Ÿ(BlueKing-BK-DBM) available.
Copyright (C) 2017-2023 THL A29 Limited, a Tencent company. All rights reserved.
Licensed under the MIT License (the "License"); you may not use this file except in compliance with the License.
You may obtain a copy of the License at https://opensource.org/licenses/MIT
Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
"""
import copy
import json
import logging
import re
from abc import ABCMeta
from collections import defaultdict
from typing import Any, Dict, List, Optional, Union

from bamboo_engine import states
from django.utils import translation
from django.utils.translation import ugettext as _
from pipeline.core.flow.activity import Service, StaticIntervalGenerator

from backend import env
from backend.components import JobApi
from backend.components.sops.client import BkSopsApi
from backend.core.encrypt.constants import AsymmetricCipherConfigType
from backend.core.encrypt.handlers import AsymmetricHandler
from backend.core.translation.constants import Language
from backend.flow.consts import DEFAULT_FLOW_CACHE_EXPIRE_TIME, SUCCESS_LIST, WriteContextOpType
from backend.ticket.constants import TicketFlowStatus
from backend.ticket.models import Flow
from backend.utils.excel import ExcelHandler
from backend.utils.redis import RedisConn

logger = logging.getLogger("flow")
cpl = re.compile("<ctx>(?P<context>.+?)</ctx>")  # éè´ªå©ªæ¨¡å¼ï¼ŒåªåŒ¹é…ç¬¬ä¸€æ¬¡å‡ºç°çš„è‡ªå®šä¹‰tag


class ServiceLogMixin:
    def log_info(self, msg: str):
        logger.info(msg, extra=self.extra_log)

    def log_error(self, msg: str):
        logger.error(msg, extra=self.extra_log)

    def log_warning(self, msg: str):
        logger.warning(msg, extra=self.extra_log)

    def log_debug(self, msg: str):
        logger.debug(msg, extra=self.extra_log)

    def log_exception(self, msg: str):
        logger.exception(msg, extra=self.extra_log)

    def log_dividing_line(self):
        logger.info("*" * 50, extra=self.extra_log)

    @property
    def runtime_attrs(self):
        """pipeline.activityå†…éƒ¨è¿è¡Œæ—¶å±æ€§"""
        return getattr(self, "_runtime_attrs", {})

    @property
    def extra_log(self):
        """Serviceè¿è¡Œæ—¶å±æ€§ï¼Œç”¨äºè·å– root_id, node_id, version_id æ³¨å…¥æ—¥å¿—"""
        return {
            "root_id": self.runtime_attrs.get("root_pipeline_id"),
            "node_id": self.runtime_attrs.get("id"),
            "version_id": self.runtime_attrs.get("version"),
        }


class BaseService(Service, ServiceLogMixin, metaclass=ABCMeta):
    """
    DB Service åŸºç±»
    """

    @classmethod
    def active_language(cls, data):
        # æ¿€æ´»å›½é™…åŒ–
        blueking_language = data.get_one_of_inputs("global_data").get("blueking_language", Language.ZH_CN.value)
        translation.activate(blueking_language)

    @classmethod
    def get_flow_output(cls, flow: Union[Flow, str]):
        """
        è·å–æµç¨‹çš„ç¼“å­˜æ•°æ®ã€‚åªå…è®¸åœ¨æµç¨‹æˆåŠŸç»“æŸåæ‰§è¡Œ
        @param flow: å½“å‰æµç¨‹
        """
        if isinstance(flow, str):
            flow = Flow.objects.get(flow_obj_id=flow)
        if flow.status != TicketFlowStatus.SUCCEEDED:
            raise ValueError(_("æµç¨‹{}ä¸ä¸ºæˆåŠŸæ€").format(flow.flow_obj_id))
        if flow.details.get("__flow_output"):
            return flow.details.get("__flow_output")

        # è·å–ç¼“å­˜æ•°æ®
        flow_cache_key, flow_sensitive_key = f"{flow.flow_obj_id}_list", f"{flow.flow_obj_id}_is_sensitive"
        flow_cache_data = [json.loads(item) for item in RedisConn.lrange(flow_cache_key, 0, -1)]
        # åˆå¹¶ç›¸åŒçš„key
        merge_data = defaultdict(list)
        for data in flow_cache_data:
            for k, v in data.items():
                merge_data[k].append(v)

        # å¦‚æœæ˜¯æ•æ„Ÿæ•°æ®ï¼Œåˆ™æ•´ä½“åŠ å¯†
        is_sensitive = int(RedisConn.get(flow_sensitive_key) or False)
        if is_sensitive:
            merge_data = json.dumps(merge_data)
            merge_data = AsymmetricHandler.encrypt(name=AsymmetricCipherConfigType.PASSWORD.value, content=merge_data)

        # å…¥åº“åˆ°flowçš„detailsä¸­ï¼Œå¹¶åˆ é™¤ç¼“å­˜key
        flow.update_details(
            __flow_output={"root_id": flow.flow_obj_id, "is_sensitive": is_sensitive, "data": merge_data}
        )
        RedisConn.delete(flow_cache_key, flow_sensitive_key)

        return flow.details["__flow_output"]

    def set_flow_output(self, root_id: str, key: Union[int, str], value: Any, is_sensitive: bool = False):
        """
        åœ¨æ•´ä¸ªæµç¨‹ä¸­å­˜å…¥ç¼“å­˜æ•°æ®ï¼Œåªå…è®¸è¿½åŠ ä¸æ”¯æŒä¿®æ”¹ï¼Œå¯¹ç›¸åŒçš„keyä¼šåˆå¹¶ä¸ºlist
        åœ¨æµç¨‹æ‰§è¡ŒæˆåŠŸåï¼Œç¼“å­˜æ•°æ®ä¼šå…¥åº“åˆ°
        @param root_id: æµç¨‹id
        @param key: ç¼“å­˜é”®å€¼
        @param value: å¯jsonåºåˆ—åŒ–çš„æ•°æ®
        @param is_sensitive: æœ¬æ¬¡ç¼“å­˜æ˜¯å¦æ˜¯æ•æ„Ÿæ•°æ®
        """
        # åºåˆ—åŒ–
        try:
            data = json.dumps({key: value})
        except TypeError:
            self.log_exception(_("è¯¥æ•°æ®{}:{}æ— æ³•è¢«åºåˆ—åŒ–ï¼Œè·³è¿‡æ­¤æ¬¡ç¼“å­˜").format(key, value))
            return
        flow_cache_key, flow_sensitive_key = f"{root_id}_list", f"{root_id}_is_sensitive"

        # ç”¨liståŸè¯­ç¼“å­˜æ•°æ®ï¼Œä¸ä¼šå‡ºç°ç«æ€
        RedisConn.lpush(flow_cache_key, data)
        # åªä¼šè®¾ç½®ä¸ºTrueï¼Œä¸ä¼šå‡ºç°ç«æ€
        if is_sensitive:
            RedisConn.set(flow_sensitive_key, 1)

        # æ¯æ¬¡ç¼“å­˜éƒ½åˆ·æ–°è¿‡æœŸæ—¶é—´ã€‚æˆ‘ä»¬è®¾ç½®ä¸€ä¸ªè¶³å¤Ÿé•¿çš„è¿‡æœŸæ—¶é—´ï¼Œå¦‚æœè¿™ä¸ªä»»åŠ¡å¤±è´¥å¾ˆä¹…ä¸å¤„ç†ï¼Œé‚£ä¹ˆæ•°æ®å°±ä¼šè‡ªåŠ¨æ¸…ç†
        RedisConn.expire(flow_cache_key, DEFAULT_FLOW_CACHE_EXPIRE_TIME)
        RedisConn.expire(flow_sensitive_key, DEFAULT_FLOW_CACHE_EXPIRE_TIME)

    def excel_download(self, root_id: str, key: Union[int, str], match_header: bool = False):
        """
        æ ¹æ®root_idä¸‹è½½Excelæ–‡ä»¶
        :param root_id: æµç¨‹id
        :param key: ç¼“å­˜é”®å€¼
        """
        # è·å–root_idç¼“å­˜æ•°æ®
        flow_details = self.get_flow_output(root_id)

        # æ£€æŸ¥ flow_details æ˜¯å¦å­˜åœ¨ä»¥åŠæ˜¯å¦åŒ…å«æ‰€éœ€çš„ key
        if not flow_details or key not in flow_details["data"]:
            self.log_error(_("ä¸‹è½½excelæ–‡ä»¶å¤±è´¥ï¼Œæœªè·å–åˆ°{}ç›¸å…³çš„æ•°æ®").format(key))
            return False

        flow_detail = flow_details["data"][key]

        # å¦‚æœ flow_detail æ˜¯ä¸€ä¸ªåŒé‡åˆ—è¡¨ï¼Œåˆ™å»æ‰ä¸€ä¸ªåˆ—è¡¨
        if isinstance(flow_detail[0], list):
            flow_detail = flow_detail[0]

        if not isinstance(flow_detail[0], dict):
            self.log_error(_("ä¸‹è½½excelæ–‡ä»¶å¤±è´¥ï¼Œè·å–æ•°æ®æ ¼å¼é”™è¯¯"))
            return False

        # å–ç¬¬ä¸€ä¸ªå­—å…¸çš„é”®ä½œä¸ºhead
        head_list = list(flow_detail[0].keys())

        excel_name = f"{root_id}.xlsx"
        # å°†æ•°æ®å­—å…¸åºåˆ—åŒ–ä¸ºexcelå¯¹è±¡
        wb = ExcelHandler.serialize(flow_detail, headers=head_list, match_header=match_header)
        # è¿”å›å“åº”
        return ExcelHandler.response(wb, excel_name)

    def execute(self, data, parent_data):
        self.active_language(data)

        kwargs = data.get_one_of_inputs("kwargs") or {}
        try:
            result = self._execute(data, parent_data)
            if result:
                self.log_info(_("[{}] è¿è¡ŒæˆåŠŸ").format(kwargs.get("node_name", self.__class__.__name__)))

            return result
        except Exception as e:  # pylint: disable=broad-except
            self.log_exception(_("[{}] å¤±è´¥: {}").format(kwargs.get("node_name", self.__class__.__name__), e))
            return False

    def _execute(self, data, parent_data):
        raise NotImplementedError()

    def schedule(self, data, parent_data, callback_data=None):
        self.active_language(data)

        kwargs = data.get_one_of_inputs("kwargs") or {}
        try:
            result = self._schedule(data, parent_data)
            return result
        except Exception as e:  # pylint: disable=broad-except
            self.log_exception(f"[{kwargs.get('node_name', self.__class__.__name__)}] failed: {e}")
            self.finish_schedule()
            return False

    def _schedule(self, data, parent_data, callback_data=None):
        self.finish_schedule()
        return True

    @staticmethod
    def splice_exec_ips_list(
        ticket_ips: Optional[Any] = None, pool_ips: Optional[Any] = None, parse_cloud: bool = False
    ) -> Union[List[Dict], List[str]]:
        """
        æ‹¼æ¥æ‰§è¡Œipåˆ—è¡¨
        @param ticket_ips: è¡¨ç¤ºå•æ®é¢„åˆ†é…å¥½çš„ipä¿¡æ¯
        @param pool_ips:   è¡¨ç¤ºä»èµ„æºæ± åˆ†é…åˆ°çš„ipä¿¡æ¯
        @param parse_cloud: æ˜¯å¦éœ€è¦è§£æäº‘åŒºåŸŸ TODO: å½“flowå…¼å®¹åï¼Œé»˜è®¤ä¸ºäº‘åŒºåŸŸ
        """

        def _format_to_str(ip: Union[str, dict]) -> Union[str, Dict]:
            """
            {"ip": "127.0.0.1", "bk_cloud_id": 0} -> "127.0.0.1"
            """
            if isinstance(ip, dict):
                if not parse_cloud:
                    return ip["ip"]
                return {"ip": ip["ip"], "bk_cloud_id": ip["bk_cloud_id"]}
            return ip

        def _format_to_list(ip: Optional[Union[list, str]]) -> list:
            # TODO æœ€ç»ˆåº”è¯¥ä½¿ç”¨ _format_to_dict ç»Ÿä¸€è¿”å› [{"ip": "127.0.0.1", "bk_cloud_id": 0 }]
            #  éœ€ flow æ•´ä½“é…åˆæ”¹é€ 
            if ip is None:
                return []
            if isinstance(ip, list):
                return [_format_to_str(_ip) for _ip in ip]
            return [_format_to_str(ip)]

        return _format_to_list(ticket_ips) + _format_to_list(pool_ips)


class BkJobService(BaseService, metaclass=ABCMeta):
    __need_schedule__ = True
    interval = StaticIntervalGenerator(5)

    @staticmethod
    def __status__(instance_id: str) -> Optional[Dict]:
        """
        è·å–ä»»åŠ¡çŠ¶æ€
        """
        payload = {
            "bk_biz_id": env.JOB_BLUEKING_BIZ_ID,
            "job_instance_id": instance_id,
            "return_ip_result": True,
        }
        resp = JobApi.get_job_instance_status(payload, raw=True)
        return resp

    def __log__(
        self,
        job_instance_id: int,
        step_instance_id: int,
        ip_dict: dict,
    ):
        """
        è·å–ä»»åŠ¡æ—¥å¿—
        """
        payload = {
            "bk_biz_id": env.JOB_BLUEKING_BIZ_ID,
            "job_instance_id": job_instance_id,
            "step_instance_id": step_instance_id,
        }
        return JobApi.get_job_instance_ip_log({**payload, **ip_dict}, raw=True)

    def __get_target_ip_context(
        self,
        job_instance_id: int,
        step_instance_id: int,
        ip_dict: dict,
        data,
        trans_data,
        write_payload_var: str,
        write_op: str,
    ):
        """
        å¯¹å•ä¸ªèŠ‚ç‚¹è·å–æ‰§è¡Œålogï¼Œå¹¶èµ‹å€¼ç»™å®šä¹‰å¥½æµç¨‹ä¸Šä¸‹æ–‡çš„trans_data
        write_op æ§åˆ¶å†™å…¥å˜é‡çš„æ–¹å¼ï¼Œrewriteæ˜¯é»˜è®¤å€¼ï¼Œä»£è¡¨è¦†ç›–å†™å…¥ï¼›appendä»£è¡¨ä»¥{"ip":xxx} å½¢å¼è¿½åŠ é‡Œé¢å˜é‡é‡Œé¢
        """
        resp = self.__log__(job_instance_id, step_instance_id, ip_dict)
        if not resp["result"]:
            # ç»“æœè¿”å›å¼‚å¸¸ï¼Œåˆ™å¼‚å¸¸é€€å‡º
            return False
        try:
            # ä»¥dictå½¢å¼è¿½åŠ å†™å…¥
            result = json.loads(re.search(cpl, resp["data"]["log_content"]).group("context"))
            if write_op == WriteContextOpType.APPEND.value:
                context = copy.deepcopy(getattr(trans_data, write_payload_var))
                ip = ip_dict["ip"]
                if context:
                    context[ip] = copy.deepcopy(result)
                else:
                    context = {ip: copy.deepcopy(result)}

                setattr(trans_data, write_payload_var, copy.deepcopy(context))

            else:
                # é»˜è®¤è¦†ç›–å†™å…¥
                setattr(trans_data, write_payload_var, copy.deepcopy(result))
            data.outputs["trans_data"] = trans_data

            return True

        except Exception as e:
            self.log_error(_("[å†™å…¥ä¸Šä¸‹æ–‡ç»“æœå¤±è´¥] failed: {}").format(e))
            return False

    def _schedule(self, data, parent_data, callback_data=None) -> bool:
        ext_result = data.get_one_of_outputs("ext_result")
        exec_ips = data.get_one_of_outputs("exec_ips")
        kwargs = data.get_one_of_inputs("kwargs")
        write_payload_var = data.get_one_of_inputs("write_payload_var")
        trans_data = data.get_one_of_inputs("trans_data")

        node_name = kwargs["node_name"]

        # åœ¨è½®è¯¢çš„æ—¶å€™ext_resultéƒ½ä¸ä¼šæ”¹å˜ï¼Œè€ƒè™‘æ”¶æ•›æ—¥å¿—
        if not kwargs.get(f"{node_name}_ext_result_cached"):
            kwargs[f"{node_name}_ext_result_cached"] = True
            self.log_info(f"[{node_name}] ext_result: {ext_result}")

        if isinstance(ext_result, bool):
            # ext_result ä¸º å¸ƒå°”ç±»å‹ è¡¨ç¤º ä¸éœ€è¦ä»»åŠ¡æ˜¯åŒæ­¥è¿›è¡Œï¼Œä¸éœ€è¦è°ƒç”¨apiå»ç›‘å¬ä»»åŠ¡çŠ¶æ€
            self.finish_schedule()
            return ext_result

        if not ext_result["result"]:
            # è°ƒç”¨ç»“æœæ£€æµ‹åˆ°å¤±è´¥
            self.log_error(f"[{node_name}] schedule  status failed: {ext_result['error']}")
            return False

        job_instance_id = ext_result["data"]["job_instance_id"]
        resp = self.__status__(job_instance_id)

        # è·å–ä»»åŠ¡çŠ¶æ€ï¼š
        # """
        # 1.æœªæ‰§è¡Œ; 2.æ­£åœ¨æ‰§è¡Œ; 3.æ‰§è¡ŒæˆåŠŸ; 4.æ‰§è¡Œå¤±è´¥; 5.è·³è¿‡; 6.å¿½ç•¥é”™è¯¯;
        # 7.ç­‰å¾…ç”¨æˆ·; 8.æ‰‹åŠ¨ç»“æŸ; 9.çŠ¶æ€å¼‚å¸¸; 10.æ­¥éª¤å¼ºåˆ¶ç»ˆæ­¢ä¸­; 11.æ­¥éª¤å¼ºåˆ¶ç»ˆæ­¢æˆåŠŸ; 12.æ­¥éª¤å¼ºåˆ¶ç»ˆæ­¢å¤±è´¥
        # """
        if not (resp["result"] and resp["data"]["finished"]):
            self.log_info(_("[{}] ä»»åŠ¡æ­£åœ¨æ‰§è¡ŒğŸ¤”").format(node_name))
            return True

        # è·å–jobçš„çŠ¶æ€
        job_status = resp["data"]["job_instance"]["status"]

        # é»˜è®¤dbmè°ƒç”¨jobæ˜¯ä¸€ä¸ªæ­¥éª¤ï¼Œæ‰€ä»¥ç»Ÿä¸€è·å–ç¬¬ä¸€ä¸ªæ­¥éª¤id
        step_instance_id = resp["data"]["step_instance_list"][0]["step_instance_id"]

        # è·å–æœ¬æ¬¡æ‰§è¡Œçš„æ‰€æœ‰ipä¿¡æ¯
        # ip_dict = {"bk_cloud_id": kwargs["bk_cloud_id"], "ip": exec_ips[0]} if exec_ips else {}
        # ip_dicts = [{"bk_cloud_id": kwargs["bk_cloud_id"], "ip": ip} for ip in exec_ips] if exec_ips else []
        ip_dicts = []
        if exec_ips:
            for i in exec_ips:
                if isinstance(i, dict) and i.get("ip") is not None and i.get("bk_cloud_id") is not None:
                    ip_dicts.append(i)
                else:
                    # å…¼å®¹ä¹‹å‰ä»£ç 
                    ip_dicts.append({"bk_cloud_id": kwargs["bk_cloud_id"], "ip": i})

        # åˆ¤æ–­æœ¬æ¬¡jobä»»åŠ¡æ˜¯å¦å¼‚å¸¸
        if job_status not in SUCCESS_LIST:
            self.log_info("{} job status: {}".format(node_name, resp))
            self.log_info(_("[{}]  ä»»åŠ¡è°ƒåº¦å¤±è´¥ğŸ˜±").format(node_name))

            # è½¬è½½jobè„šæœ¬èŠ‚ç‚¹æŠ¥é”™æ—¥å¿—ï¼Œå…¼å®¹å¤šIPæ‰§è¡Œåœºæ™¯çš„æ—¥å¿—è¾“å‡º
            if ip_dicts:
                for ip_dict in ip_dicts:
                    resp = self.__log__(job_instance_id, step_instance_id, ip_dict)
                    if resp.get("result"):
                        self.log_error(f"{ip_dict}:{resp['data']['log_content']}")

            self.finish_schedule()
            return False

        self.log_info(_("[{}]ä»»åŠ¡è°ƒåº¦æˆåŠŸğŸ¥³ï¸").format(node_name))
        if not write_payload_var:
            self.finish_schedule()
            return True

        # å†™å…¥ä¸Šä¸‹æ–‡ï¼Œæ”¯æŒå¤šIPä¼ å…¥ä¸Šä¸‹æ–‡æ•æ‰åœºæ™¯
        # å†™å…¥ä¸Šä¸‹æ–‡çš„ä½ç½®æ˜¯trans_data.{write_payload_var} å±æ€§ä¸Šï¼Œåˆ†åˆ«æ‰§è¡Œè¦†ç›–å†™å…¥å’Œè¿½åŠ å†™å…¥
        # è¦†ç›–å†™å…¥æ˜¯ä¼šç›´æ¥èµ‹å€¼ç»™ä¸Šä¸‹æ–‡å±æ€§ä¸Šï¼Œä¸ç®¡ä¹‹å‰æœ‰ä»€ä¹ˆå€¼ï¼Œè¿™æ˜¯é»˜è®¤å†™å…¥ WriteContextOpType.REWRITE
        # è¿½åŠ å†™å…¥æ˜¯ç‰¹æ®Šè¡Œä¸ºï¼Œå¦‚æœæƒ³IPæ—¥å¿—ç»“æœéƒ½å†™å…¥ï¼Œå¯ä»¥é€‰æ‹©è¿½åŠ å†™å…¥ï¼Œä¸Šä¸‹æ–‡å˜æˆlistï¼Œæ¯ä¸ªå…ƒç´ æ˜¯{"ip":"log"} WriteContextOpType.APPEND
        self.log_info(_("[{}]è¯¥èŠ‚ç‚¹éœ€è¦è·å–æ‰§è¡Œåæ—¥å¿—ï¼Œèµ‹å€¼åˆ°æµç¨‹ä¸Šä¸‹æ–‡").format(node_name))

        is_false = False
        for ip_dict in ip_dicts:
            if not self.__get_target_ip_context(
                job_instance_id=job_instance_id,
                step_instance_id=step_instance_id,
                ip_dict=ip_dict,
                data=data,
                trans_data=trans_data,
                write_payload_var=write_payload_var,
                write_op=kwargs.get("write_op", WriteContextOpType.REWRITE.value),
            ):
                self.log_error(_("[{}] è·å–æ‰§è¡Œåå†™å…¥æµç¨‹ä¸Šä¸‹æ–‡å¤±è´¥ï¼Œip:[{}]").format(node_name, ip_dict["ip"]))
                is_false = True

        if is_false:
            self.finish_schedule()
            return False

        self.finish_schedule()
        return True


class BkSopsService(BaseService, metaclass=ABCMeta):
    __need_schedule__ = True
    interval = StaticIntervalGenerator(5)
    """
    å®šä¹‰è°ƒç”¨æ ‡å‡†è¿ç»´çš„åŸºç±»
    """

    def _schedule(self, data, parent_data, callback_data=None):
        kwargs = data.get_one_of_inputs("kwargs")
        bk_biz_id = kwargs["bk_biz_id"]
        task_id = data.get_one_of_outputs("task_id")
        param = {"bk_biz_id": bk_biz_id, "task_id": task_id, "with_ex_data": True}
        rp_data = BkSopsApi.get_task_status(param)
        state = rp_data.get("state", states.RUNNING)
        if state == states.FINISHED:
            self.finish_schedule()
            self.log_info("run success~")
            return True

        if state in [states.FAILED, states.REVOKED, states.SUSPENDED]:
            if state == states.FAILED:
                self.log_error(_("ä»»åŠ¡å¤±è´¥"))
            else:
                self.log_error(_("ä»»åŠ¡çŠ¶æ€å¼‚å¸¸{}").format(state))
            # æŸ¥è¯¢å¼‚å¸¸æ—¥å¿—
            self.log_error(rp_data.get("ex_data", _("æŸ¥è¯¢æ—¥å¿—å¤±è´¥")))
            return False
